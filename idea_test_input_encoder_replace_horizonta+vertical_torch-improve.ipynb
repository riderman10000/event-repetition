{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing out the encoder replacement in this section and see if it works or not\n",
    "\n",
    "##### testing by adding the horizontal and vertical sections together \n",
    "\n",
    "##### the inference speed and model size decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2 \n",
    "import argparse \n",
    "import torch \n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from repnet import utils, plots\n",
    "from repnet.model import RepNet\n",
    "\n",
    "import event_tools.tools as tl \n",
    "import visualization as vz \n",
    "import event_tools.e2v as e2v \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_class = \"class2\"\n",
    "# event_user = \"user02_fluorescent\"\n",
    "event_user = \"user02_fluorescent_led\"\n",
    "# event_user = \"user02_lab\"\n",
    "# event_user = \"user02_led\"\n",
    "# event_user = \"user02_natural\"\n",
    "\n",
    "event_csv_file_name = f\"./event_csv/split_data/{event_class}/{event_user}.csv\"\n",
    "\n",
    "count_data = {\n",
    "    \"class\": event_class,\n",
    "    \"condition\": event_user,\n",
    "    \"count\": 0,\n",
    "}\n",
    "\n",
    "# output count info \n",
    "out_csv_file = \"encoder_replaced_output.csv\"\n",
    "\n",
    "if os.path.exists(out_csv_file):\n",
    "    try: \n",
    "        out_df = pd.read_csv(out_csv_file)\n",
    "    except: \n",
    "        out_df = pd.DataFrame([count_data])\n",
    "else: \n",
    "    with open(out_csv_file, \"w\") as f:\n",
    "        f.write(\"class,condition,count\\n\")\n",
    "    out_df = pd.read_csv(out_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>condition</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class4</td>\n",
       "      <td>user02_led</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class4</td>\n",
       "      <td>user02_led</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class3</td>\n",
       "      <td>user02_led</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class3</td>\n",
       "      <td>user02_lab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class3</td>\n",
       "      <td>user02_natural</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class       condition  count\n",
       "0  class4      user02_led      3\n",
       "1  class4      user02_led      3\n",
       "2  class3      user02_led     11\n",
       "3  class3      user02_lab     11\n",
       "4  class3  user02_natural      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_df = pd.concat([out_df, pd.DataFrame(columns=[\"class\", \"condition\", \"count\"])])\n",
    "\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = './'\n",
    "events = tl.load_event_data(\n",
    "    # './event_csv/split_data/class7/user02_lab.csv')  # Replace with your actual file path\n",
    "    event_csv_file_name) # Replace with your actual file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repnet model variables \n",
    "weights = './pytorch_weights.pth'\n",
    "device = 'cuda'\n",
    "strides = [1, 2, 3, 4 , 8]\n",
    "\n",
    "\n",
    "OUT_VISUALIZATIONS_DIR = './visualization/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiking variables\n",
    "\n",
    "width=128\n",
    "height=128\n",
    "\n",
    "# horizontal_member_potential = torch.zeros(width).to(device)\n",
    "# vertical_member_potential = torch.zeros(width).to(device) \n",
    "\n",
    "horizontal_member_potential = torch.zeros(width).reshape((1, -1)).to(device)        \n",
    "vertical_member_potential = torch.zeros(width).reshape((1, -1)).to(device)\n",
    "\n",
    "\n",
    "# horizontal_neurons = np.zeros(width).reshape((1, -1))\n",
    "# vertical_neurons = np.zeros(height).reshape((1, -1))\n",
    "\n",
    "# horizontal_member_potential = torch.zeros((6, width), device=device) \n",
    "# vertical_member_potential = torch.zeros((64, width), device=device) \n",
    "\n",
    "# horizontal_neurons = torch.zeros((64, width), device=device).reshape((1, -1))\n",
    "# vertical_neurons = torch.zeros((64, height), device=device).reshape((1, -1))\n",
    "\n",
    "\n",
    "\n",
    "decay = .009 # 0.009\n",
    "batch_size = 1 #  64 #  \n",
    "random_wt_2d = lambda _row, _col: torch.ones((_row, _col), device=device) * 0.5 # np.round(np.random.rand(_row, _col), 3)\n",
    "weight_input_horizontal_hidden = random_wt_2d(batch_size, height) # np.ones((1, height)) * weight_scale_1\n",
    "weight_input_horizontal_hidden_1 = random_wt_2d(height, height//2) # np.ones((height, height//2)) * weight_scale_1\n",
    "\n",
    "# not considering the height and width for the image as they are symmetrical \n",
    "weight_input_vertical_hidden = random_wt_2d(batch_size, height) # np.ones((1, height)) * weight_scale_1\n",
    "weight_input_vertical_hidden_1 = random_wt_2d(height, height//2) # np.ones((height, height//2)) * weight_scale_1\n",
    "\n",
    "\n",
    "\n",
    "peak = 255 \n",
    "spike = 1 # 255 \n",
    "threshold = 0.8 \n",
    "\n",
    "fps = 120\n",
    "frame_per_second  = 1/fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2v.save_event_as_video(events,name=\"event.mp4\", frame_per_second=frame_per_second, decay=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read frames and apply preprocessing \n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((112, 112)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5),      \n",
    "])\n",
    "\n",
    "raw_frames, frames = [], [] \n",
    "\n",
    "for idx, frame_image in enumerate(\n",
    "    e2v.get_event_frame(events, frame_per_second = frame_per_second, \n",
    "        decay= decay)):\n",
    "    frames.append(torch.from_numpy(frame_image.copy()).float().to(device))\n",
    "    frame_image = frame_image #.T\n",
    "    cv2.imshow('raw' , frame_image)\n",
    "    \n",
    "    frame_image = np.clip(frame_image * 255, 0, 255).astype(np.uint8)\n",
    "    frame_image = cv2.cvtColor(frame_image, cv2.COLOR_GRAY2BGR)\n",
    "    # cv2.imshow('test', cv2.resize(frame_image, [frame_image.shape[0] * 10, frame_image.shape[0] * 10] )) \n",
    "    \n",
    "    # if(cv2.waitKey(1) == ord('q')):\n",
    "       \n",
    "    #     break \n",
    "    raw_frames.append(frame_image)\n",
    "    # frame = transform(frame_image)\n",
    "    # frames.append(frame)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 642, list)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_frames), len(frames), type(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rlwagun/.cache/torch/hub/huggingface_pytorch-image-models_main\n",
      "/tmp/ipykernel_2486680/1317078289.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RepNet(\n",
       "  (temporal_conv): Sequential(\n",
       "    (0): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(3, 1, 1), dilation=(3, 1, 1))\n",
       "    (1): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveMaxPool3d(output_size=(None, 1, 1))\n",
       "    (4): Flatten(start_dim=2, end_dim=4)\n",
       "  )\n",
       "  (tsm_conv): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (period_length_head): Sequential(\n",
       "    (0): TranformerLayer(\n",
       "      (input_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (transformer_layer): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=512, out_features=32, bias=True)\n",
       "  )\n",
       "  (periodicity_head): Sequential(\n",
       "    (0): TranformerLayer(\n",
       "      (input_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (transformer_layer): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = RepNet()\n",
    "state_dict = torch.load(weights)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "if hasattr(model, 'encoder'):\n",
    "    del model.encoder\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage: 196.45 MB\n",
      "CPU RAM Usage: 1109.16 MB\n",
      "Model Size: 78.13 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "# GPU Memory Usage\n",
    "gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert to MB\n",
    "print(f\"GPU Memory Usage: {gpu_memory:.2f} MB\")\n",
    "\n",
    "# CPU RAM Usage\n",
    "ram_usage = psutil.Process().memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "print(f\"CPU RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())  # Model parameters\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())  # Model buffers (like BatchNorm stats)\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 * 1024)  # Convert to MB\n",
    "    return total_size_mb\n",
    "\n",
    "model_size = get_model_size(model)\n",
    "print(f\"Model Size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# frames = torch.stack(frames)\n",
    "len(frames)\n",
    "\n",
    "\n",
    "spikes_collection = []\n",
    "# Process frames sequentially while optimizing computations\n",
    "# for idx in range(frames.shape[0]):  # Stride-based loop\n",
    "for idx in range(len(frames)):  # Stride-based loop\n",
    "    frame_image = frames[idx]\n",
    "\n",
    "    # Update potentials sequentially\n",
    "    horizontal_member_potential.mul_(decay).add_(weight_input_horizontal_hidden @ frame_image)\n",
    "    vertical_member_potential.mul_(decay).add_(weight_input_vertical_hidden @ frame_image.T)\n",
    "\n",
    "    # Generate spikes\n",
    "    spike_mask_h = horizontal_member_potential > (spike * threshold)\n",
    "    spike_mask_v = vertical_member_potential > (spike * threshold)\n",
    "\n",
    "    horizontal_neurons = spike_mask_h.float()\n",
    "    vertical_neurons = spike_mask_v.float()\n",
    "\n",
    "    horizontal_member_potential.masked_fill_(spike_mask_h, 0).relu_()\n",
    "    vertical_member_potential.masked_fill_(spike_mask_v, 0).relu_()\n",
    "\n",
    "    # Collect spikes\n",
    "    spikes_collection.append(torch.cat([horizontal_neurons, vertical_neurons], dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted period length: 0.4 sec (~53 frames) with confidence 0.90, using a stride of 4.\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "import torch\n",
    "\n",
    "# Initialize potentials\n",
    "horizontal_member_potential = torch.zeros((1, width), device=device)\n",
    "vertical_member_potential = torch.zeros((1, width), device=device)\n",
    "\n",
    "best_stride, best_confidence, best_period_length, best_period_count, best_periodicity_score, best_embeddings = None, None, None, None, None, None\n",
    "\n",
    "for stride in strides:\n",
    "    # apply stride \n",
    "    stride_frames = spikes_collection[::stride]\n",
    "    stride_frames = stride_frames[:(len(stride_frames) // 64) * 64]\n",
    "    if len(stride_frames) < 64:\n",
    "        continue # Skip this stride if there are not enough frames \n",
    "        \n",
    "    raw_period_length, raw_periodicity_score, embeddings = [], [], []\n",
    "\n",
    "    # iterate in steps of 64 frames \n",
    "    for start_idx in range(0, len(stride_frames) - 63, 64):\n",
    "        window_frames = stride_frames[start_idx:start_idx + 64] \n",
    "        # Convert to tensor and reshape\n",
    "        spikes_0 = torch.stack(window_frames).view(-1, 64, 128 * 2).to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            batch_period_length, batch_periodicity, batch_embeddings = model(spikes_0)\n",
    "\n",
    "                # Store results\n",
    "            raw_period_length.append(batch_period_length[0].cpu())\n",
    "            raw_periodicity_score.append(batch_periodicity[0].cpu())\n",
    "            embeddings.append(batch_embeddings[0].cpu())\n",
    "\n",
    "    # Post-process results\n",
    "    # if raw_period_length and raw_periodicity_score:\n",
    "    raw_period_length, raw_periodicity_score, embeddings = map(torch.cat, [raw_period_length, raw_periodicity_score, embeddings])\n",
    "    confidence, period_length, period_count, periodicity_score = model.get_counts(raw_period_length, raw_periodicity_score, stride)\n",
    "\n",
    "    if best_confidence is None or confidence > best_confidence:\n",
    "        best_stride, best_confidence, best_period_length, best_period_count, best_periodicity_score, best_embeddings = stride, confidence, period_length, period_count, periodicity_score, embeddings\n",
    "\n",
    "# Final check\n",
    "if best_stride is None:\n",
    "    raise RuntimeError('Stride values too large; no 64-frame chunk could be sampled. Try different --strides values.')\n",
    "\n",
    "print(f'Predicted period length: {best_period_length/fps:.1f} sec (~{int(best_period_length)} frames) with confidence {best_confidence:.2f}, using a stride of {best_stride}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# # works but really slow \n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Initialize potentials\n",
    "# horizontal_member_potential = torch.zeros((1, width), device=device)\n",
    "# vertical_member_potential = torch.zeros((1, width), device=device)\n",
    "\n",
    "# best_stride, best_confidence, best_period_length, best_period_count, best_periodicity_score, best_embeddings = None, None, None, None, None, None\n",
    "\n",
    "# spikes_collection = []\n",
    "# # Process frames sequentially while optimizing computations\n",
    "# # for idx in range(frames.shape[0]):  # Stride-based loop\n",
    "# for idx in range(len(frames)):  # Stride-based loop\n",
    "#     frame_image = frames[idx]\n",
    "\n",
    "#     # Update potentials sequentially\n",
    "#     horizontal_member_potential.mul_(decay).add_(weight_input_horizontal_hidden @ frame_image)\n",
    "#     vertical_member_potential.mul_(decay).add_(weight_input_vertical_hidden @ frame_image.T)\n",
    "\n",
    "#     # Generate spikes\n",
    "#     spike_mask_h = horizontal_member_potential > (spike * threshold)\n",
    "#     spike_mask_v = vertical_member_potential > (spike * threshold)\n",
    "\n",
    "#     horizontal_neurons = spike_mask_h.float()\n",
    "#     vertical_neurons = spike_mask_v.float()\n",
    "\n",
    "#     horizontal_member_potential.masked_fill_(spike_mask_h, 0).relu_()\n",
    "#     vertical_member_potential.masked_fill_(spike_mask_v, 0).relu_()\n",
    "\n",
    "#     # Collect spikes\n",
    "#     spikes_collection.append(torch.cat([horizontal_neurons, vertical_neurons], dim=-1))\n",
    "\n",
    "\n",
    "# for stride in strides:\n",
    "#     # apply stride \n",
    "#     stride_frames = spikes_collection[::stride]\n",
    "#     stride_frames = stride_frames[:(len(stride_frames) // 64) * 64]\n",
    "#     if len(stride_frames) < 64:\n",
    "#         continue # Skip this stride if there are not enough frames \n",
    "        \n",
    "#     raw_period_length, raw_periodicity_score, embeddings = [], [], []\n",
    "\n",
    "#     # # Process batch inference every 64 frames\n",
    "#     # if len(stride_frames) == 64:\n",
    "#     for i in range(0, len(stride_frames) - 64  +1 , 64):\n",
    "#         batch_frames = stride_frames[i:i + 64]\n",
    "#         batch_frames = torch.stack(batch_frames).view(-1, 64, 128 * 2).to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         with torch.no_grad():\n",
    "#             batch_period_length, batch_periodicity, batch_embeddings = model(batch_frames)\n",
    "\n",
    "#         # Store results\n",
    "#         raw_period_length.append(batch_period_length[0].cpu())\n",
    "#         raw_periodicity_score.append(batch_periodicity[0].cpu())\n",
    "#         embeddings.append(batch_embeddings[0].cpu())\n",
    "#     # with torch.no_grad():\n",
    "#     #     spikes_0 = torch.stack(stride_frames).view(-1, 64, 128 * 2).to(device)\n",
    "#     #     batch_period_length, batch_periodicity, batch_embeddings = model(spikes_0)\n",
    "\n",
    "#     #     # Store results\n",
    "#     #     raw_period_length.append(batch_period_length[0].cpu())\n",
    "#     #     raw_periodicity_score.append(batch_periodicity[0].cpu())\n",
    "#     #     embeddings.append(batch_embeddings[0].cpu())\n",
    "\n",
    "#     # Post-process results\n",
    "#     if raw_period_length and raw_periodicity_score:\n",
    "#         raw_period_length, raw_periodicity_score, embeddings = map(torch.cat, [raw_period_length, raw_periodicity_score, embeddings])\n",
    "#         confidence, period_length, period_count, periodicity_score = model.get_counts(raw_period_length, raw_periodicity_score, stride)\n",
    "\n",
    "#         if best_confidence is None or confidence > best_confidence:\n",
    "#             best_stride, best_confidence, best_period_length, best_period_count, best_periodicity_score, best_embeddings = stride, confidence, period_length, period_count, periodicity_score, embeddings\n",
    "\n",
    "# # Final check\n",
    "# if best_stride is None:\n",
    "#     raise RuntimeError('Stride values too large; no 64-frame chunk could be sampled. Try different --strides values.')\n",
    "\n",
    "# print(f'Predicted period length: {best_period_length/fps:.1f} sec (~{int(best_period_length)} frames) with confidence {best_confidence:.2f}, using a stride of {best_stride}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Generate video with counts\n",
    "rep_frames = plots.plot_repetitions(raw_frames[:len(best_period_count)], best_period_count.tolist(), best_periodicity_score.tolist() if not True else None)\n",
    "video = cv2.VideoWriter(os.path.join(OUT_VISUALIZATIONS_DIR, 'repetitions.mp4'), cv2.VideoWriter_fourcc(*'mp4v'), fps, rep_frames[0].shape[:2][::-1])\n",
    "for frame in rep_frames:\n",
    "    video.write(frame)\n",
    "video.release()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_period_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_frames), len(best_period_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_period_count.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_period_count.tolist()[-1]\n",
    "round(best_period_count.tolist()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0208, 0.0417, 0.0625, 0.0833, 0.1042, 0.1250, 0.1458, 0.1667, 0.1875,\n",
       "        0.2083, 0.2292, 0.2500, 0.2708, 0.2917, 0.3125, 0.3333, 0.3542, 0.3750,\n",
       "        0.3958, 0.4167, 0.4375, 0.4583, 0.4792, 0.5000, 0.5208, 0.5417, 0.5625,\n",
       "        0.5833, 0.6042, 0.6250, 0.6458, 0.6667, 0.6875, 0.7083, 0.7292, 0.7500,\n",
       "        0.7708, 0.7917, 0.8125, 0.8333, 0.8526, 0.8718, 0.8910, 0.9103, 0.9295,\n",
       "        0.9487, 0.9679, 0.9872, 1.0064, 1.0256, 1.0449, 1.0641, 1.0833, 1.1026,\n",
       "        1.1218, 1.1410, 1.1603, 1.1795, 1.1987, 1.2179, 1.2372, 1.2564, 1.2756,\n",
       "        1.2949, 1.3141, 1.3333, 1.3526, 1.3718, 1.3910, 1.4103, 1.4295, 1.4487,\n",
       "        1.4679, 1.4872, 1.5064, 1.5256, 1.5449, 1.5641, 1.5833, 1.6026, 1.6218,\n",
       "        1.6410, 1.6603, 1.6795, 1.6987, 1.7179, 1.7372, 1.7564, 1.7756, 1.7949,\n",
       "        1.8141, 1.8333, 1.8526, 1.8718, 1.8910, 1.9103, 1.9295, 1.9487, 1.9679,\n",
       "        1.9872, 2.0064, 2.0256, 2.0449, 2.0641, 2.0833, 2.1026, 2.1218, 2.1410,\n",
       "        2.1603, 2.1795, 2.1987, 2.2179, 2.2372, 2.2564, 2.2756, 2.2949, 2.3141,\n",
       "        2.3333, 2.3526, 2.3718, 2.3910, 2.4103, 2.4295, 2.4487, 2.4679, 2.4872,\n",
       "        2.5064, 2.5256, 2.5449, 2.5641, 2.5833, 2.6026, 2.6218, 2.6410, 2.6603,\n",
       "        2.6795, 2.6987, 2.7179, 2.7372, 2.7564, 2.7756, 2.7949, 2.8141, 2.8333,\n",
       "        2.8526, 2.8718, 2.8910, 2.9103, 2.9295, 2.9487, 2.9679, 2.9872, 3.0064,\n",
       "        3.0256, 3.0449, 3.0641, 3.0833, 3.1026, 3.1218, 3.1410, 3.1603, 3.1795,\n",
       "        3.1987, 3.2179, 3.2372, 3.2564, 3.2756, 3.2949, 3.3141, 3.3333, 3.3526,\n",
       "        3.3718, 3.3910, 3.4103, 3.4295, 3.4487, 3.4679, 3.4872, 3.5064, 3.5256,\n",
       "        3.5449, 3.5641, 3.5833, 3.6026, 3.6218, 3.6410, 3.6603, 3.6795, 3.6987,\n",
       "        3.7179, 3.7372, 3.7564, 3.7756, 3.7949, 3.8141, 3.8333, 3.8526, 3.8718,\n",
       "        3.8910, 3.9103, 3.9295, 3.9487, 3.9679, 3.9872, 4.0064, 4.0256, 4.0449,\n",
       "        4.0641, 4.0833, 4.1026, 4.1218, 4.1410, 4.1603, 4.1795, 4.1987, 4.2179,\n",
       "        4.2372, 4.2564, 4.2756, 4.2949, 4.3141, 4.3333, 4.3526, 4.3718, 4.3910,\n",
       "        4.4103, 4.4295, 4.4487, 4.4679, 4.4872, 4.5064, 4.5256, 4.5449, 4.5641,\n",
       "        4.5833, 4.6026, 4.6218, 4.6410, 4.6603, 4.6795, 4.6987, 4.7179, 4.7372,\n",
       "        4.7564, 4.7756, 4.7949, 4.8141, 4.8333, 4.8526, 4.8718, 4.8910, 4.9103,\n",
       "        4.9295, 4.9487, 4.9679, 4.9872, 5.0050, 5.0229, 5.0408, 5.0586, 5.0765,\n",
       "        5.0943, 5.1122, 5.1300, 5.1479, 5.1658, 5.1836, 5.2015, 5.2193, 5.2372,\n",
       "        5.2550, 5.2729, 5.2908, 5.3086, 5.3265, 5.3443, 5.3622, 5.3800, 5.3979,\n",
       "        5.4158, 5.4336, 5.4515, 5.4693, 5.4872, 5.5050, 5.5229, 5.5408, 5.5586,\n",
       "        5.5765, 5.5943, 5.6122, 5.6300, 5.6479, 5.6658, 5.6836, 5.7015, 5.7193,\n",
       "        5.7372, 5.7550, 5.7729, 5.7908, 5.8086, 5.8265, 5.8443, 5.8622, 5.8800,\n",
       "        5.8979, 5.9158, 5.9336, 5.9515, 5.9693, 5.9872, 6.0050, 6.0229, 6.0408,\n",
       "        6.0586, 6.0765, 6.0943, 6.1122, 6.1300, 6.1479, 6.1658, 6.1836, 6.2015,\n",
       "        6.2193, 6.2372, 6.2550, 6.2729, 6.2908, 6.3086, 6.3265, 6.3443, 6.3622,\n",
       "        6.3800, 6.3979, 6.4158, 6.4336, 6.4515, 6.4693, 6.4872, 6.5050, 6.5229,\n",
       "        6.5408, 6.5586, 6.5765, 6.5943, 6.6122, 6.6300, 6.6479, 6.6658, 6.6836,\n",
       "        6.7015, 6.7193, 6.7372, 6.7550, 6.7729, 6.7908, 6.8086, 6.8265, 6.8443,\n",
       "        6.8622, 6.8800, 6.8979, 6.9158, 6.9336, 6.9515, 6.9693, 6.9872, 7.0050,\n",
       "        7.0229, 7.0408, 7.0586, 7.0765, 7.0943, 7.1122, 7.1300, 7.1479, 7.1658,\n",
       "        7.1836, 7.2015, 7.2193, 7.2372, 7.2550, 7.2729, 7.2908, 7.3086, 7.3265,\n",
       "        7.3443, 7.3622, 7.3800, 7.3979, 7.4158, 7.4336, 7.4515, 7.4693, 7.4872,\n",
       "        7.5050, 7.5229, 7.5408, 7.5586, 7.5765, 7.5943, 7.6122, 7.6300, 7.6479,\n",
       "        7.6658, 7.6836, 7.7015, 7.7193, 7.7372, 7.7550, 7.7729, 7.7908, 7.8086,\n",
       "        7.8265, 7.8443, 7.8622, 7.8800, 7.8979, 7.9158, 7.9336, 7.9515, 7.9693,\n",
       "        7.9872, 8.0050, 8.0229, 8.0408, 8.0586, 8.0765, 8.0943, 8.1122, 8.1300,\n",
       "        8.1479, 8.1658, 8.1836, 8.2015, 8.2193, 8.2372, 8.2550, 8.2729, 8.2908,\n",
       "        8.3086, 8.3265, 8.3443, 8.3622, 8.3800, 8.3979, 8.4158, 8.4336, 8.4515,\n",
       "        8.4693, 8.4872, 8.5050, 8.5229, 8.5408, 8.5586, 8.5765, 8.5943, 8.6122,\n",
       "        8.6300, 8.6479, 8.6658, 8.6836, 8.7015, 8.7193, 8.7372, 8.7550, 8.7729,\n",
       "        8.7908, 8.8086, 8.8265, 8.8443, 8.8622, 8.8800, 8.8979, 8.9158, 8.9336,\n",
       "        8.9515, 8.9693, 8.9872, 9.0050, 9.0229, 9.0408, 9.0586, 9.0765, 9.0943,\n",
       "        9.1122, 9.1300, 9.1479, 9.1658, 9.1836, 9.2015, 9.2193, 9.2372, 9.2550,\n",
       "        9.2729, 9.2908, 9.3086, 9.3265, 9.3443, 9.3622, 9.3800, 9.3979, 9.4158,\n",
       "        9.4336, 9.4515, 9.4693, 9.4872, 9.5050, 9.5229, 9.5408, 9.5586])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_period_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data[\"count\"] = round(best_period_count.tolist()[-1])\n",
    "try:\n",
    "    if (out_df.loc[(out_df['class'] == event_class) & (out_df['condition'] == event_user), 'count']):\n",
    "        out_df.loc[(out_df['class'] == event_class) & (out_df['condition'] == event_user), 'count'] = count_data[\"count\"]\n",
    "except:\n",
    "    out_df = pd.concat([out_df, pd.DataFrame([count_data])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv(out_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>condition</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class4</td>\n",
       "      <td>user02_led</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class4</td>\n",
       "      <td>user02_led</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class3</td>\n",
       "      <td>user02_led</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class3</td>\n",
       "      <td>user02_lab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class3</td>\n",
       "      <td>user02_natural</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class       condition  count\n",
       "0  class4      user02_led      3\n",
       "1  class4      user02_led      3\n",
       "2  class3      user02_led     11\n",
       "3  class3      user02_lab     11\n",
       "0  class3  user02_natural      3"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df\n",
    "# out_df.loc[(out_df['class'] == event_class) & (out_df['condition'] == event_user), 'count'] = count_data[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage: 523.25 MB\n",
      "CPU RAM Usage: 1670.50 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "# GPU Memory Usage\n",
    "gpu_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert to MB\n",
    "print(f\"GPU Memory Usage: {gpu_memory:.2f} MB\")\n",
    "\n",
    "# CPU RAM Usage\n",
    "ram_usage = psutil.Process().memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "print(f\"CPU RAM Usage: {ram_usage:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save plots and video with counts to ./visualization/...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate plots and videos\n",
    "print(f'Save plots and video with counts to {OUT_VISUALIZATIONS_DIR}...')\n",
    "os.makedirs(OUT_VISUALIZATIONS_DIR, exist_ok=True)\n",
    "dist = torch.cdist(best_embeddings, best_embeddings, p=2)**2\n",
    "tsm_img = plots.plot_heatmap(dist.numpy(), log_scale=True)\n",
    "pca_img = plots.plot_pca(best_embeddings.numpy())\n",
    "cv2.imwrite(os.path.join(OUT_VISUALIZATIONS_DIR, 'tsm.png'), tsm_img)\n",
    "cv2.imwrite(os.path.join(OUT_VISUALIZATIONS_DIR, 'pca.png'), pca_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
